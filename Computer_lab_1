#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Jan 31 10:22:58 2023

@author: emildamirov
"""

import pandas as pd
from scipy.stats import norm
from matplotlib import cm
import numpy as np
import matplotlib.pyplot as plt
import math as m
import os
import datetime  as dt
import statistics as st


os.chdir('/Users/emildamirov/Downloads')
df = pd.read_excel('DataLab1.xlsx')
df["Date"] = pd.to_datetime(df["Date"])
df.set_index('Date', inplace=True)
print(df)


#%%

start = dt.datetime(2005, 1, 1)
end = dt.datetime(2008,12,31)
our_sample = df[start:end]

alpha = 0.99 # VaR/ES Level
our_sample['losses'] = our_sample['PL']*-1 # Don't forget the minus sign if we want the loss distribution
sorted_losses = our_sample.sort_values('losses', ascending=False)

T =  len(sorted_losses.index)# Get the number of returns
idx = m.floor((1-alpha)*T+1) # Calculate the index on the ordered data 

#%%

# Task 1

rT = 500 # Size of rolling sample

our_sample.insert(2,"VaR",np.nan)

for j in range(rT,T): 
    roll_sample = our_sample.iloc[j-rT:j,1]
    our_sample.iloc[j,2] = np.quantile(roll_sample,0.99)
    

fig,ax=plt.subplots(nrows=1,ncols=1)
ax.plot(our_sample['VaR'])
plt.xlabel('Date',fontsize=12)
plt.ylabel('VaR (%)',fontsize=12)
plt.title('HS on 500 days rolling',fontsize=16)
ax.tick_params(axis='x',labelrotation=90,labelsize=9)
    

#%%

# Task 2a

s_mean = our_sample['losses'].iloc[0:500].mean()
our_sample['losses_new'] = our_sample['losses'] - s_mean
    

# Task 2b 
    
eta = our_sample.losses_new # Setting innovation equal to Loss is ok since daily returns close to zero and hard to predict
sigma = [np.var(eta.iloc[0:500])] # Sigma_0 in slides
lambda_par = 0.94 # RiskMetrics


for j in range(1,T):
    sigma.append((1-lambda_par)*eta[j-1]**2+lambda_par*sigma[j-1])

sigma = np.array(sigma)
annualized_percent_vol = (500*sigma)**0.5  # this is task 2c

fig,ax=plt.subplots()
ax.plot(annualized_percent_vol)
plt.title('Volatility from EWMA model',fontsize=14)

our_sample['std'] = sigma

# Task 2c

sigma_con = []

for i in sigma:
    sigma_con.append(m.sqrt(i))
    
our_sample['std_con'] = sigma_con

#%%
# Task 3

scaled_losses = np.zeros((rT,len(our_sample.losses_new)-rT))
scaled_losses.shape

for j in range(rT, T):
    scaled_losses[:,j-rT] = our_sample.std_con[j]/our_sample.std_con[j-rT:j]*our_sample.losses_new[j-rT:j]  
    
matrix = pd.DataFrame(scaled_losses)

#%%
# Task 4 

vw_VaR99 = []
vw_ES99 = [] 
our_sample.insert(6,"VW_VaR",np.nan)

for j in range(rT,T):
    vw_VaR99.append(np.quantile(matrix[j-rT],0.99))  
    interim = matrix[j-rT]
    vw_ES99.append(np.mean(interim[interim >vw_VaR99[j-rT]]))
    our_sample.iloc[j,6] = vw_VaR99[j-rT]    

    
fig,ax=plt.subplots()
ax.plot(our_sample['losses'][our_sample['losses']>0],label='Losses')
ax.plot(our_sample['VaR'],label='BHS')
ax.plot(our_sample['VW_VaR'],label='VWHS')

plt.xlabel('Date',fontsize=12)
plt.ylabel('VIX (%)',fontsize=12)
plt.title('BHS and VWHS on 500 days rolling',fontsize=16)
plt.legend()
fig.set_size_inches(12, 6)

#%%

# Task 5a
plt.hist(our_sample['losses'], edgecolor='black')

import numpy as np
import pylab
import scipy.stats as stats


# Mean & Variance
print("Sample mean {:.2f}".format(st.mean(our_sample['losses'])))
print("Sample variance {:.2f}".format(st.variance(our_sample['losses'])))

# mean, var, skew, kurt = t.stats(df, moments='mvsk')

# Histogram
import seaborn as sns
plot1 = sns.distplot(our_sample['losses'])
plt.hist(our_sample.losses, 30, density=True)

# Jarque-Berra
from scipy.stats import jarque_bera
data = our_sample['losses']
statistic,pvalue = jarque_bera(data)
print('Statistic=%.3f, P_value=%.3f\n' % (statistic, pvalue))

#Skewness
print("Sample skewness {:.2f}".format(stats.skew(our_sample.losses)))

#Kurtosis
print("Sample kurtosis {:.2f}".format(stats.kurtosis(our_sample.losses)))

# ('Losses have heavy tails and are right skewed, which will underestimate VaR and ES if we assume normality. Instead we use T-distribution')


# Task 5b
nu,loc,scale = stats.t.fit(our_sample.losses)
print('nu={:.2f} loc={:.4f}, and scale={:.4f}'.format(nu,loc,scale))


Par_t = pd.DataFrame()
Par_t['losses'] = our_sample['losses']
Par_t.insert(1,"location",np.nan)
Par_t.insert(2,"scale",np.nan)
Par_t.insert(3,"degr_fr",np.nan)
#Par_t['degr_fr'][500:1007] = 2.1

for j in range(rT,T): 
    roll_sample = our_sample.iloc[j-rT:j,1]
    Par_t.iloc[j,1] = stats.t.fit(roll_sample)[1]
    Par_t.iloc[j,2] = stats.t.fit(roll_sample)[2]
    Par_t.iloc[j,3] = stats.t.fit(roll_sample)[0]


# Normal
Par_n = pd.DataFrame()
Par_n['losses'] = our_sample['losses']
Par_n.insert(1,"mean",np.nan)
Par_n.insert(2,"st_dev",np.nan)


for j in range(rT,T): 
    roll_sample = our_sample.iloc[j-rT:j,1]
    Par_n.iloc[j,1] = st.mean(roll_sample)
    Par_n.iloc[j,2] = st.stdev(roll_sample)


# Task 5c
for j in range(rT,T):
    if Par_t['degr_fr'][j]< 2:
        Par_t['degr_fr'][j] = 2.1


Par_t.insert(4,"VaR",np.nan)
Par_t.insert(5,"ES",np.nan)
Par_n.insert(3,"VaR",np.nan)
Par_n.insert(4,"ES",np.nan)

Par_n['VaR'] = our_sample['VaR']

for j in range(rT,T):
    
    # T-distribution
    
    alpha = 0.99
    mu = Par_n['mean'][j]
    nu = Par_t['degr_fr'][j]
    std = Par_n['st_dev'][j]
    sig = m.sqrt(nu/(nu-2))*std
    Par_t.iloc[j,4]= mu+m.sqrt((nu-2)/nu)*sig*stats.t.ppf(alpha,nu)
    part1 = m.sqrt((nu-2)/nu)*sig*stats.t.pdf(stats.t.ppf(alpha,nu),nu)
    part2 = 1/(1-alpha)*(nu+stats.t.ppf(alpha,nu)**2)/(nu-1)
    Par_t.iloc[j,5] = mu+part1*part2 # Slide 13 Video lecture 7
    
    # N-distribution
    
    roll_sample = our_sample.iloc[j-rT:j,1]
    sorted_losses = roll_sample.sort_values(ascending=False) 
    Par_n.iloc[j,4] = np.mean(sorted_losses[:4])
    
    
# Task 6

pot = pd.DataFrame()
pot['losses'] = our_sample['losses']
pot.insert(1,"VaR",np.nan)
pot.insert(2,"ES",np.nan)


for j in range(rT,T):
    roll_sample = our_sample.iloc[j-rT:j,1]
    alpha = 0.99
    u = np.quantile(roll_sample,0.95)
    large_losses = roll_sample[roll_sample>u]
    (xsi,loc,beta) = stats.genpareto.fit(large_losses,floc=0) # Setting localtion to zero
    N = len(roll_sample)
    N_u = len(large_losses)
    middlePart = (N/N_u)*(1-alpha) # The "middle part" of the VaR equation on slide 18 just to improve readability
    pot['VaR'].iloc[j] = u+(beta/xsi)*((middlePart**(-xsi))-1)
    pot['ES'].iloc[j] = (pot['VaR'].iloc[j]+beta-xsi*u)/(1-xsi)
    
    
fig,ax=plt.subplots()
ax.plot(our_sample['losses'][our_sample['losses']>0],label='Losses')
ax.plot(our_sample['VaR'],label='BHS')
ax.plot(our_sample['VW_VaR'],label='VWHS')
ax.plot(pot['VaR'],label='POT')

plt.xlabel('Date',fontsize=12)
plt.ylabel('VIX (%)',fontsize=12)
plt.title('BHS,VWHS, POT on 500 days rolling',fontsize=16)
plt.legend()
fig.set_size_inches(12, 6)    
    

    